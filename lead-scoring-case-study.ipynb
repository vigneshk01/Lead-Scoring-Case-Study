{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead Scoring Case Study \n",
    "\n",
    "- Supervised Learning Algorithm - Logistic Regression [ Classification ]\n",
    "- Programming Language - Python\n",
    "- Developed by UV, VK, VY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries numpy, pandas, matplotlib, seaborn. \n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "%matplotlib inline\n",
    "\n",
    "# Set custom display properties in pandas\n",
    "pd.set_option(\"display.max_rows\", 900)\n",
    "pd.set_option(\"display.max_columns\", 900) \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score,precision_score, roc_auc_score, confusion_matrix, f1_score, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To distinguish numerical columns either as categorical/discrete or non categorical and return as dict\n",
    "def classify_feature_dtype(df,cols):\n",
    "    d_categories = {'int_cat': [], \"float_ts\":[] }\n",
    "    for col in cols:\n",
    "        if (len(df[col].unique()) < 20):\n",
    "            d_categories['int_cat'].append(col)\n",
    "        else:\n",
    "            d_categories['float_ts'].append(col)\n",
    "    return d_categories\n",
    "\n",
    "# Print all statistical information for a given set of columns\n",
    "def show_stats(df, cols):\n",
    "    for col in list(cols):\n",
    "        print(\"Total Nulls: {0},\\nMode: {1}\".format(df[col].isna().sum(), df[col].mode()[0]))\n",
    "        if len(df[col].unique()) < 50:\n",
    "            print(\"\\nUnique: {0}\\n\".format(df[col].unique()))\n",
    "        if (df[col].dtype == int) or (df[col].dtype == float):\n",
    "            print(\"Median   : {0}, \\nVariance: {1}, \\n\\nDescribe: {2} \\n\".format(df[col].median(), df[col].var(), df[col].describe()))\n",
    "        print(\"ValueCounts: {0} \\n\\n\\n\".format((df[col].value_counts(normalize=True) * 100).head(5)))\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "# Return the percentage of null values in each columns in a dataframe\n",
    "def check_cols_null_pct(df):\n",
    "    df_non_na = df.count() / len(df)  # Ratio of non null values\n",
    "    df_na_pct = (1 - df_non_na) * 100 # Find the Percentage of null values\n",
    "    return df_na_pct.sort_values(ascending=False) # Sort the resulting values in descending order\n",
    "\n",
    "# Generates charts based on the data type of the cols, as part of the univariate analysis \n",
    "# it takes dataframe, columns, train data 0,1, and feature type as args.\n",
    "def univariate_plots(df, cols, target=None, ftype=None, l_dict = None):\n",
    "    for col in cols:\n",
    "        #generate plots and graphs for category type. (generates piechart, countplot, boxplot / if training data is provided it generates bar chart instead)\n",
    "        if ftype == \"categorical\":\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "                        \n",
    "            col_idx = 0\n",
    "            axs[col_idx].pie(x=df[col].value_counts().head(15), labels=df[col].value_counts().head(15).index.str[:15], autopct=\"%1.1f%%\", \n",
    "                    radius=1, textprops={\"fontsize\": 10, \"color\": \"Black\"}, startangle=90, rotatelabels=False)\n",
    "            axs[col_idx].set_title(\"PieChart of {0}\".format(col), y=1); plt.xticks(rotation=45); plt.ylabel(\"Percentage\")\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "            sns.countplot(data=df, y=col, order=df[col].value_counts().index, palette=\"viridis\",  ax=axs[col_idx])\n",
    "            if (l_dict is not None) and (l_dict.get(col) is not None):\n",
    "                axs[col_idx].legend([ f'{k} - {v}' for k,v in l_dict[col].items()])\n",
    "            axs[col_idx].set_title(\"Countplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            col_idx += 1\n",
    "            ax = sns.barplot(data=df, x=df[col].str[:15], y=target, palette=\"viridis\",  ax=axs[col_idx], errwidth=0)\n",
    "            for i in ax.containers:\n",
    "                ax.bar_label(i,)\n",
    "            axs[col_idx].set_title('Barplot against target'); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            plt.suptitle(\"Univariate analysis of {0}\".format(col), fontsize=12, y=0.95)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        #generate plots and graphs for numerical types. (generates boxplot, histplot, kdeplot, scatterplot)\n",
    "        elif ftype == \"non_categorical\":        \n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "            \n",
    "            col_idx = 0\n",
    "            \n",
    "            sns.boxplot(data=df, y=col, palette=\"viridis\", flierprops=dict( marker=\"o\", markersize=6, markerfacecolor=\"red\", markeredgecolor=\"black\"),\n",
    "                        medianprops=dict(linestyle=\"-\", linewidth=3, color=\"#FF9900\"), whiskerprops=dict(linestyle=\"-\", linewidth=2, color=\"black\"),\n",
    "                        capprops=dict(linestyle=\"-\", linewidth=2, color=\"black\"), ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"Boxplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            axs[col_idx].hist(data=df, x=col, label=col)\n",
    "            axs[col_idx].set_title(\"Histogram of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            sns.kdeplot(df[col], shade=True, ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"KDE plot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            sns.scatterplot(df[col], ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"Scatterplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            plt.suptitle(\"Univariate analysis of {0}\".format(col), fontsize=12, y=0.95)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "# Perform Outlier analysis on the given dataframe.\n",
    "# Find Lower threshold, Upper threshold and IQR values. \n",
    "# Return the Result as a dataframe. \n",
    "# find_outlier = True argument: restricts the output df to outlier columns. whereas find_outlier = False: returns results for all columns\n",
    "def get_extremeval_threshld(df, find_outlier=False):\n",
    "    outlier_df = pd.DataFrame(columns=[i for i in df.columns if find_outlier == True], data=None)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        thirdq, firstq = df[col].quantile(0.75), df[col].quantile(0.25)\n",
    "        iqr = 1.5 * (thirdq - firstq)\n",
    "        extvalhigh, extvallow = iqr + thirdq, firstq - iqr\n",
    "        \n",
    "        if find_outlier == True:\n",
    "            dfout = df.loc[(df[col] > extvalhigh) | (df[col] < extvallow)]\n",
    "            dfout = dfout.assign(name=col, thresh_low=extvallow, thresh_high=extvalhigh)\n",
    "        else:\n",
    "            dfout = pd.DataFrame([[col, extvallow, extvalhigh]], columns=['name', 'thresh_low', 'thresh_high'])\n",
    "            \n",
    "        outlier_df = pd.concat([outlier_df, dfout])\n",
    "    # outlier_df = outlier_df.reset_index(drop=True)\n",
    "    outlier_df = outlier_df.set_index('name',drop=True)\n",
    "    return outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df = pd.read_csv('Leads.csv')\n",
    "lead_score_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicates row validation by id cols\n",
    "print(f\"{lead_score_df.index.is_unique}, {lead_score_df.columns.is_unique}, {lead_score_df['Prospect ID'].is_unique}, {lead_score_df['Lead Number'].is_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "lead_score_df = lead_score_df.drop(columns=['Prospect ID', 'I agree to pay the amount through cheque', 'Last Notable Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns that are too long\n",
    "lead_score_df = lead_score_df.rename(columns={'Total Time Spent on Website':'ttime_on_site', 'Page Views Per Visit':'pg_view_pv', 'How did you hear about X Education':'info_abt_X_Edu', 'What is your current occupation':'curr_occupation',\n",
    "    'What matters most to you in choosing a course':'reason_behind_course', 'Receive More Updates About Our Courses':'more_course_updates', 'Update me on Supply Chain Content':'supply_chain_info', 'Get updates on DM Content':'get_dm',\n",
    "    'Asymmetrique Activity Index':'asym_activ_idx', 'Asymmetrique Profile Index':'asym_prof_idx', 'Asymmetrique Activity Score':'asym_activ_score', 'Asymmetrique Profile Score':'asym_prof_score',\n",
    "    'A free copy of Mastering The Interview':'avail_free_copy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unnecessary space in columns with underscore and covert it to lower case\n",
    "lead_score_df.columns = lead_score_df.columns.str.replace(pat=' ',repl='_', regex=True)\n",
    "lead_score_df.columns = lead_score_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.head(1)\n",
    "lead_score_df.dtypes\n",
    "print(f'{lead_score_df.shape}, {lead_score_df.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null val percentage\n",
    "check_cols_null_pct(lead_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace select string with nan\n",
    "lead_score_df = lead_score_df.replace(to_replace=['select','Select'], value=np.nan)\n",
    "\n",
    "# validate select str is replaced\n",
    "[i for i in lead_score_df.columns if 'select' in (lead_score_df[i].astype(str).str.lower()).str.findall('select').value_counts().index.map(''.join).to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check constant features that has only one values \n",
    "from fast_ml.utilities import display_all\n",
    "from fast_ml.feature_selection import get_constant_features\n",
    "\n",
    "constant_features = get_constant_features(lead_score_df)\n",
    "constant_features.head(10)\n",
    "\"','\".join(constant_features['Var'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the constant_features\n",
    "lead_score_df = lead_score_df.drop(['magazine', 'more_course_updates', 'supply_chain_info', 'get_dm', 'x_education_forums', \n",
    "                                    'newspaper', 'do_not_call', 'newspaper_article', 'digital_advertisement', 'through_recommendations', 'search'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtypes \n",
    "obj_cols = lead_score_df.select_dtypes(include=object).columns\n",
    "lead_score_df[obj_cols] = lead_score_df[obj_cols].astype(dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null val percentage\n",
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct\n",
    "lead_score_df = lead_score_df.drop(null_pct[null_pct > 40].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct\n",
    "na_cols = null_pct[null_pct > 0].index\n",
    "na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.describe(include=np.number)\n",
    "lead_score_df.describe(exclude=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical values using mode, if a particular value in that column has higher frequency say > 50%\n",
    "for i in lead_score_df.select_dtypes(include='category'):\n",
    "    temp = lead_score_df[i].value_counts(normalize=True, ascending=False) * 100\n",
    "    if temp.iloc[0] > 50:\n",
    "        lead_score_df[i] = lead_score_df[i].fillna(temp.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct[null_pct > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- specialization   36.580\n",
    "- tags             36.288\n",
    "- totalvisits       1.483\n",
    "- pg_view_pv        1.483\n",
    "- last_activity     1.115\n",
    "- lead_source       0.390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate plots\n",
    "univariate_plots(lead_score_df, lead_score_df.select_dtypes(include=[int,float]).columns, ftype='non_categorical', target='converted')\n",
    "univariate_plots(lead_score_df, lead_score_df.select_dtypes(exclude=[int,float]).columns, ftype='categorical', target='converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bivariate plots\n",
    "sns.pairplot(lead_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate plots\n",
    "plt.figure(figsize = (10, 6)) # Size of the figure\n",
    "sns.heatmap(lead_score_df.select_dtypes(exclude='category').corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Analysis and Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_val_df = get_extremeval_threshld(df=lead_score_df.select_dtypes(exclude=['category','object']) )\n",
    "ex_val_df\n",
    "lead_score_df.describe(percentiles=[.05,.1,.2,.5,.8,.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Outliers by setting either thresh low or thresh low for both extremes \n",
    "lower_cutoff = ex_val_df.loc['pg_view_pv','thresh_low']\n",
    "lead_score_df['pg_view_pv'] = np.where((lead_score_df['pg_view_pv'] < lower_cutoff), lower_cutoff, lead_score_df['pg_view_pv'])\n",
    "upper_cutoff = ex_val_df.loc['pg_view_pv','thresh_high']\n",
    "lead_score_df['pg_view_pv'] = np.where((lead_score_df['pg_view_pv'] > upper_cutoff), upper_cutoff, lead_score_df['pg_view_pv'])\n",
    "\n",
    "# Fix Outliers by setting either thresh low or thresh low for both extremes\n",
    "lower_cutoff = ex_val_df.loc['totalvisits','thresh_low']\n",
    "lead_score_df['totalvisits'] = np.where((lead_score_df['totalvisits'] < lower_cutoff), lower_cutoff, lead_score_df['totalvisits'])\n",
    "upper_cutoff = ex_val_df.loc['totalvisits','thresh_high']\n",
    "lead_score_df['totalvisits'] = np.where((lead_score_df['totalvisits'] > upper_cutoff), upper_cutoff, lead_score_df['totalvisits'])\n",
    "\n",
    "# Fix Outliers by setting either thresh low or thresh low for both extremes\n",
    "lower_cutoff = ex_val_df.loc['ttime_on_site','thresh_low']\n",
    "lead_score_df['ttime_on_site'] = np.where((lead_score_df['ttime_on_site'] < lower_cutoff), lower_cutoff, lead_score_df['ttime_on_site'])\n",
    "upper_cutoff = ex_val_df.loc['ttime_on_site','thresh_high']\n",
    "lead_score_df['ttime_on_site'] = np.where((lead_score_df['ttime_on_site'] > upper_cutoff), upper_cutoff, lead_score_df['ttime_on_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df['totalvisits'] = lead_score_df['totalvisits'].replace(to_replace=np.nan, value=lead_score_df['totalvisits'].mean())\n",
    "lead_score_df['pg_view_pv'] = lead_score_df['pg_view_pv'].replace(to_replace=np.nan, value=lead_score_df['pg_view_pv'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Yes, No with 1 and 0\n",
    "lead_score_df = lead_score_df.replace(to_replace=['Yes', 'No'], value=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Imbalance & Conversion Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imbalance\n",
    "imbalance_ratio = sum(lead_score_df['converted'] == 1)/sum(lead_score_df['converted'] == 0)*100\n",
    "print(f'{round(imbalance_ratio, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion Ratio \n",
    "converted = (sum(lead_score_df['converted'])/len(lead_score_df['converted'].index))*100\n",
    "print(f'{round(converted, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Code to be removed !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_encoding:\n",
    "cols_to_le = [i for i in lead_score_df.columns if ((lead_score_df[i].dtype == 'category') and len(lead_score_df[i].unique()) > 5)]\n",
    "cols_to_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_le = lead_score_df[cols_to_le].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_le.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ls_df = lead_score_df[lead_score_df.columns.difference(cols_to_le)]\n",
    "new_ls_df = new_ls_df.merge(right=df_le, right_index=True, left_index=True)\n",
    "new_ls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cols_null_pct(new_ls_df)\n",
    "# new_ls_df = new_ls_df[(new_ls_df.notna()).all(axis=1)]\n",
    "# check_cols_null_pct(new_ls_df)\n",
    "# sorted([f'{i} - {new_ls_df[i].unique()}' for i in new_ls_df.columns])\n",
    "new_ls_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_encoding:\n",
    "cols_to_de = [i for i in new_ls_df.columns if ((new_ls_df[i].dtype == 'category') and len(new_ls_df[i].unique()) <= 5)]\n",
    "cols_to_de\n",
    "new_ls_df = pd.get_dummies(new_ls_df, columns=cols_to_de, drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ls_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_train_pred_fn(fX_train, fy_train, fcol, fcutoff, fres=None):\n",
    "    \n",
    "    fX_train_sm = sm.add_constant(fX_train[fcol])\n",
    "    \n",
    "    if fres is None:\n",
    "        flogm = sm.GLM(fy_train, fX_train_sm, family = sm.families.Binomial())\n",
    "        fres = flogm.fit()\n",
    "        \n",
    "    fy_train_pred = fres.predict(fX_train_sm)\n",
    "    fy_train_pred = fy_train_pred.values.reshape(-1)\n",
    "    fy_train_pred_final = pd.DataFrame({'Converted':fy_train.values, 'Conv_Prob':fy_train_pred})\n",
    "    fy_train_pred_final['ID'] = fy_train.index\n",
    "    fy_train_pred_final['predicted'] = fy_train_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_train_pred,fy_train_pred_final\n",
    "\n",
    "def logreg_metrics_fn(fy_train_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_train_pred_final.Converted, fy_train_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_train_pred_final.Converted, fy_train_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_VIF_score_fn(fX_train, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_train[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_train[fcol].values, i) for i in range(fX_train[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_ls_df.drop(['converted'], axis=1)\n",
    "y = new_ls_df['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_scale = ['lead_number', 'totalvisits', 'ttime_on_site', 'pg_view_pv']\n",
    "to_scale = ['lead_number', 'totalvisits', 'ttime_on_site', 'pg_view_pv', 'lead_source', 'last_activity', 'country', 'specialization', 'curr_occupation', 'tags']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[to_scale] = scaler.fit_transform(X_train[to_scale],y_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')        # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_train_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_train_pred_final\n",
    "print('\\nVIF Score:')                        # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('lead_origin_Quick Add Form', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_train_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_train_pred_final\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('reason_behind_course_Flexibility & Convenience')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_train_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_train_pred_final\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('avail_free_copy_1')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_train_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_train_pred_final\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('lead_origin_Lead Import')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_train_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_train_pred_final\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.3 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = roc_curve( actual, probs, drop_intermediate = False )\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, drop_intermediate = False )\n",
    "draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\n",
    "recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)\n",
    "plt.plot(thresholds, p[:-1], \"b\")\n",
    "plt.plot(thresholds, r[:-1], \"r\")\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation on Test Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "X_test[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_sm = sm.add_constant(X_test[col])\n",
    "# y_test_pred = res.predict(X_test_sm)\n",
    "# y_test_pred[:10]\n",
    "\n",
    "# # Converting y_pred to a dataframe which is an array\n",
    "# y_pred_1 = pd.DataFrame(y_test_pred)\n",
    "# # Let's see the head\n",
    "# y_pred_1.head()\n",
    "\n",
    "# # Converting y_test to dataframe\n",
    "# y_test_df = pd.DataFrame(y_test)\n",
    "# # Putting CustID to index\n",
    "# y_test_df['ID'] = y_test_df.index\n",
    "\n",
    "# # Removing index for both dataframes to append them side by side \n",
    "# y_pred_1.reset_index(drop=True, inplace=True)\n",
    "# y_test_df.reset_index(drop=True, inplace=True)\n",
    "# # Appending y_test_df and y_pred_1\n",
    "# y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n",
    "# y_pred_final.head()\n",
    "\n",
    "# # Renaming the column \n",
    "# y_pred_final= y_pred_final.rename(columns={ 0 : 'Conv_Prob'})\n",
    "# y_pred_final\n",
    "# # Rearranging the columns\n",
    "# y_pred_final = y_pred_final.reindex(['ID','converted','Conv_Prob'], axis=1)\n",
    "# y_pred_final\n",
    "\n",
    "# y_pred_final['final_predicted'] = y_pred_final.Conv_Prob.map(lambda x: 1 if x > 0.3 else 0)\n",
    "# y_pred_final.head()\n",
    "\n",
    "\n",
    "# confusion2 = confusion_matrix(y_pred_final.converted, y_pred_final.final_predicted )\n",
    "# confusion2\n",
    "\n",
    "# accuracy_score(y_pred_final.converted, y_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation on Test Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = 0.3\n",
    "# res, y_train_pred,y_test_pred_final = logreg_train_pred_fn(X_test, y_test, col, cutoff, res)\n",
    "# confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "# vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "# print('Model Summary:')          # Model Summary:\n",
    "# res.summary()\n",
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_test_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_test_pred_final\n",
    "# print('\\nVIF Score:')            # VIF Score:\n",
    "# vif\n",
    "# print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "# confusion\n",
    "# print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_test_pred_fn(fX_test, fy_test, fcol, fcutoff, fres):\n",
    "    \n",
    "    fX_test_sm = sm.add_constant(fX_test[fcol])\n",
    "    fy_test_pred = fres.predict(fX_test_sm)\n",
    "    fy_test_pred = fy_test_pred.values.reshape(-1)\n",
    "    fy_test_pred_final = pd.DataFrame({'Converted':fy_test.values, 'Conv_Prob':fy_test_pred})\n",
    "    fy_test_pred_final['ID'] = fy_test.index\n",
    "    fy_test_pred_final['predicted'] = fy_test_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_test_pred,fy_test_pred_final\n",
    "\n",
    "def logreg_test_metrics_fn(fy_test_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_test_pred_final.Converted, fy_test_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_test_pred_final.Converted, fy_test_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_test_VIF_score_fn(fX_test, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_test[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_test[fcol].values, i) for i in range(fX_test[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, y_test_pred, y_test_pred_final = logreg_test_pred_fn(X_test, y_test, col, cutoff, res)\n",
    "confusion, accuracy = logreg_metrics_fn(y_test_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_test, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "y_test_pred\n",
    "print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "y_test_pred_final\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfull",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

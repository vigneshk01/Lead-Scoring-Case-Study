{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead Scoring Case Study \n",
    "\n",
    "- Supervised Learning Algorithm - Logistic Regression [ Classification ]\n",
    "- Programming Language - Python\n",
    "- Developed by UV, VK, VY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization - Preprocessing - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Package Imports and Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries numpy, pandas, matplotlib, seaborn. \n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mplib\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "%matplotlib inline\n",
    "\n",
    "# Set custom display properties in pandas\n",
    "pd.set_option(\"display.max_rows\", 900)\n",
    "pd.set_option(\"display.max_columns\", 900) \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary machine learning packages (sklearn, statsmodel) for performing logistic regression\n",
    "import statsmodels.api as sm \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score,precision_score, roc_auc_score, confusion_matrix, f1_score, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset from leads.csv\n",
    "lead_score_df = pd.read_csv('Leads.csv')\n",
    "lead_score_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocesing - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Custom Functions for Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To distinguish numerical columns either as categorical/discrete or non categorical and return as dict\n",
    "def classify_feature_dtype(df, cols):\n",
    "    d_categories = {'int_cat': [], \"float_ts\":[] }\n",
    "    for col in cols:\n",
    "        if (len(df[col].unique()) < 20):\n",
    "            d_categories['int_cat'].append(col)\n",
    "        else:\n",
    "            if not isinstance(df[col][df[col].notna()].unique()[0], str):\n",
    "                d_categories['float_ts'].append(col)\n",
    "            else:\n",
    "                d_categories['int_cat'].append(col)\n",
    "    return d_categories\n",
    "\n",
    "# Print all statistical information for a given set of columns\n",
    "def show_stats(df, cols):\n",
    "    for col in list(cols):\n",
    "        print(\"Total Nulls: {0},\\nMode: {1}\".format(df[col].isna().sum(), df[col].mode()[0]))\n",
    "        if len(df[col].unique()) < 50:\n",
    "            print(\"\\nUnique: {0}\\n\".format(df[col].unique()))\n",
    "        if (df[col].dtype == int) or (df[col].dtype == float):\n",
    "            print(\"Median   : {0}, \\nVariance: {1}, \\n\\nDescribe: {2} \\n\".format(df[col].median(), df[col].var(), df[col].describe()))\n",
    "        print(\"ValueCounts: {0} \\n\\n\\n\".format((df[col].value_counts(normalize=True) * 100).head(5)))\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "# Return the percentage of null values in each columns in a dataframe\n",
    "def check_cols_null_pct(df):\n",
    "    df_non_na = df.count() / len(df)  # Ratio of non null values\n",
    "    df_na_pct = (1 - df_non_na) * 100 # Find the Percentage of null values\n",
    "    return df_na_pct.sort_values(ascending=False) # Sort the resulting values in descending order\n",
    "\n",
    "# Generates charts based on the data type of the cols, as part of the univariate analysis \n",
    "# it takes dataframe, columns, train data 0,1, and feature type as args.\n",
    "def univariate_plots(df, cols, target=None, ftype=None, l_dict = None):\n",
    "    for col in cols:\n",
    "        #generate plots and graphs for category type. (generates piechart, countplot, boxplot / if training data is provided it generates bar chart instead)\n",
    "        if ftype == \"categorical\":\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    " \n",
    "            col_idx = 0\n",
    "            axs[col_idx].pie(x=df[col].value_counts().head(12), labels=df[col].value_counts().head(12).index.str[:10], autopct=\"%1.1f%%\", \n",
    "                    radius=1, textprops={\"fontsize\": 10, \"color\": \"Black\"}, startangle=90, rotatelabels=False, )\n",
    "            axs[col_idx].set_title(\"PieChart of {0}\".format(col), y=1); plt.xticks(rotation=45); plt.ylabel(\"Percentage\")\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "            sns.countplot(data=df, y=col, order=df[col].value_counts().head(15).index, palette=\"viridis\",  ax=axs[col_idx])\n",
    "            if (l_dict is not None) and (l_dict.get(col) is not None):\n",
    "                axs[col_idx].legend([ f'{k} - {v}' for k,v in l_dict[col].items()])\n",
    "            axs[col_idx].set_title(\"Countplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col); plt.ylabel(\"Count\")\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            col_idx += 1\n",
    "            ax = sns.barplot(data=df, x=df[col].str[:10], y=target, order=df[col].value_counts().index.str[:10], palette=\"viridis\",  ax=axs[col_idx], errwidth=0)\n",
    "            for i in ax.containers:\n",
    "                ax.bar_label(i,)\n",
    "            axs[col_idx].set_title('Barplot against target'); plt.xticks(rotation=90); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            plt.suptitle(\"Univariate analysis of {0}\".format(col), fontsize=12, y=0.95)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "            plt.show();\n",
    "            plt.clf()\n",
    "\n",
    "        #generate plots and graphs for numerical types. (generates boxplot, histplot, kdeplot, scatterplot)\n",
    "        elif ftype == \"non_categorical\":        \n",
    "            fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "            \n",
    "            col_idx = 0\n",
    "            \n",
    "            sns.boxplot(data=df, y=col, palette=\"viridis\", flierprops=dict(marker=\"o\", markersize=6, markerfacecolor=\"red\", markeredgecolor=\"black\"),\n",
    "                        medianprops=dict(linestyle=\"-\", linewidth=3, color=\"#FF9900\"), whiskerprops=dict(linestyle=\"-\", linewidth=2, color=\"black\"),\n",
    "                        capprops=dict(linestyle=\"-\", linewidth=2, color=\"black\"), ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"Boxplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            axs[col_idx].hist(data=df, x=col, label=col)\n",
    "            axs[col_idx].set_title(\"Histogram of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            sns.kdeplot(df[col], shade=True, ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"KDE plot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "            \n",
    "            col_idx += 1\n",
    "\n",
    "            sns.scatterplot(df[col], ax=axs[col_idx])\n",
    "            axs[col_idx].set_title(\"Scatterplot of {0}\".format(col)); plt.xticks(rotation=45); plt.xlabel(col)\n",
    "            fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "            plt.suptitle(\"Univariate analysis of {0}\".format(col), fontsize=12, y=0.95)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "# Perform Outlier analysis on the given dataframe.\n",
    "# Find Lower threshold, Upper threshold and IQR values. \n",
    "# Return the Result as a dataframe. \n",
    "# find_outlier = True argument: restricts the output df to outlier columns. whereas find_outlier = False: returns results for all columns\n",
    "def get_extremeval_threshld(df, find_outlier=False):\n",
    "    outlier_df = pd.DataFrame(columns=[i for i in df.columns if find_outlier == True], data=None)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        thirdq, firstq = df[col].quantile(0.75), df[col].quantile(0.25)\n",
    "        iqr = 1.5 * (thirdq - firstq)\n",
    "        extvalhigh, extvallow = iqr + thirdq, firstq - iqr\n",
    "        \n",
    "        if find_outlier == True:\n",
    "            dfout = df.loc[(df[col] > extvalhigh) | (df[col] < extvallow)]\n",
    "            dfout = dfout.assign(name=col, thresh_low=extvallow, thresh_high=extvalhigh)\n",
    "        else:\n",
    "            dfout = pd.DataFrame([[col, extvallow, extvalhigh]], columns=['name', 'thresh_low', 'thresh_high'])\n",
    "            \n",
    "        outlier_df = pd.concat([outlier_df, dfout])\n",
    "    # outlier_df = outlier_df.reset_index(drop=True)\n",
    "    outlier_df = outlier_df.set_index('name',drop=True)\n",
    "    return outlier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning, Column Renaming, Missing Value Imputation, Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicates row validation by id cols\n",
    "print(f\"{lead_score_df.index.is_unique}, {lead_score_df.columns.is_unique}, {lead_score_df['Prospect ID'].is_unique}, {lead_score_df['Lead Number'].is_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "lead_score_df = lead_score_df.drop(columns=['Prospect ID','Lead Number', 'I agree to pay the amount through cheque', 'Last Notable Activity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns that are too long\n",
    "lead_score_df = lead_score_df.rename(columns={'Total Time Spent on Website':'ttime_on_site', 'Page Views Per Visit':'pg_view_pv', 'How did you hear about X Education':'info_abt_X_Edu', 'What is your current occupation':'curr_occupation',\n",
    "    'What matters most to you in choosing a course':'reason_behind_course', 'Receive More Updates About Our Courses':'more_course_updates', 'Update me on Supply Chain Content':'supply_chain_info', 'Get updates on DM Content':'get_dm',\n",
    "    'Asymmetrique Activity Index':'asym_activ_idx', 'Asymmetrique Profile Index':'asym_prof_idx', 'Asymmetrique Activity Score':'asym_activ_score', 'Asymmetrique Profile Score':'asym_prof_score',\n",
    "    'A free copy of Mastering The Interview':'avail_free_copy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unnecessary space in columns with underscore and covert it to lower case\n",
    "lead_score_df.columns = lead_score_df.columns.str.replace(pat=' ',repl='_', regex=True)\n",
    "lead_score_df.columns = lead_score_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape and size of the data frame\n",
    "lead_score_df.head(1)\n",
    "lead_score_df.dtypes\n",
    "print(f'{lead_score_df.shape}, {lead_score_df.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null val percentage\n",
    "# After checking the null value percentage for all the features\n",
    "# We could see that there are many features that have more than 45% of non values\n",
    "\n",
    "check_cols_null_pct(lead_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace select string with nan\n",
    "lead_score_df = lead_score_df.replace(to_replace=['select','Select'], value=np.nan)\n",
    "\n",
    "# validate select str is replaced\n",
    "[i for i in lead_score_df.columns if 'select' in (lead_score_df[i].astype(str).str.lower()).str.findall('select').value_counts().index.map(''.join).to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Constant Feature Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check constant features that has only one values\n",
    "# In the given data set there are a lot of features that have only single value as a category\n",
    "# These are called as constant features and these features are of little relevance for the machine learning model hence we dropped those features\n",
    " \n",
    "from fast_ml.utilities import display_all\n",
    "from fast_ml.feature_selection import get_constant_features\n",
    "\n",
    "constant_features = get_constant_features(lead_score_df)\n",
    "constant_features.head(10)\n",
    "\"','\".join(constant_features['Var'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the constant_features\n",
    "lead_score_df = lead_score_df.drop(['magazine', 'more_course_updates', 'supply_chain_info', 'get_dm', 'x_education_forums', \n",
    "                                    'newspaper', 'do_not_call', 'newspaper_article', 'digital_advertisement', 'through_recommendations', 'search'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtypes\n",
    "# Convert the data type of certain features from object to category type\n",
    "\n",
    "obj_cols = lead_score_df.select_dtypes(include='object').columns\n",
    "lead_score_df[obj_cols] = lead_score_df[obj_cols].astype(dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null val percentage\n",
    "# Since we have replaced select as null value for certain columns we are seeing increase in the null value percentage for those features\n",
    "# Therefore we dropped all the features that have more than 40 of null values\n",
    "\n",
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct\n",
    "lead_score_df = lead_score_df.drop(null_pct[null_pct > 40].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct\n",
    "na_cols = null_pct[null_pct > 0].index\n",
    "na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.describe(include=np.number)\n",
    "lead_score_df.describe(exclude=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical values using mode, if a particular value in that column has higher frequency say > 50%\n",
    "for i in lead_score_df.select_dtypes(include='category'):\n",
    "    temp = lead_score_df[i].value_counts(normalize=True, ascending=False) * 100\n",
    "    if temp.iloc[0] > 50:\n",
    "        lead_score_df[i] = lead_score_df[i].fillna(temp.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there are only two columns that have more than 36 off missing values tags and specialization\n",
    "\n",
    "null_pct = check_cols_null_pct(lead_score_df)\n",
    "null_pct[null_pct > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Visualization - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# univariate plots#\n",
    "# Now we perform Univariate analysis on both categorical and numerical variables\n",
    "dtype_dict = classify_feature_dtype(lead_score_df, lead_score_df.columns )\n",
    "univariate_plots(lead_score_df, dtype_dict['float_ts'], ftype='non_categorical', target='converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The converter column has more ones than zeros\n",
    "- The box plot for total visits shows that there are outliers and the majority of the total visits fall between zero and 30\n",
    "- The box plot for page view per visit shows that there are outliers majority of them fall between zero and 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# univariate plots\n",
    "cols = dtype_dict['int_cat'].copy()\n",
    "cols.remove('converted')\n",
    "univariate_plots(lead_score_df, cols, ftype='categorical', target='converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The categorical analysis\n",
    " - More than 50 per cent of the users are by originated landing page following which the apis have 38 of leeds\n",
    " - When it comes to lead origin conversion rates lead add form has higher conversion rates holistically all have similar probability rate\n",
    " \n",
    " - When it comes to lead source 30 are from google twenty seven percent are from direct traffic 19 are from olak chat\n",
    " \n",
    " - For the do not email feature 90 of them chose no while 7 of them chose yes therefore majority of them are interested in the edtech platform\n",
    " - Also the people who have said no also the users who said no have a higher percent of conversion rate\n",
    "  \n",
    " - The last activity of majority of the users say 38 of users are email opened followed by sms sent therefore we can say that\n",
    " - Majority of the users are active on email conversations\n",
    "  \n",
    " - Majority of the users are from india\n",
    " - We can we can assume that air tech has a higher popularity in india or higher interest in india higher demand in india\n",
    " \n",
    " - Among the employed users most of the interested users have finance management as a specialization followed by human resource management and marketing management almost all specialization has a similar conversion rate\n",
    " \n",
    " - Majority of the users are unemployed followed by working professionals\n",
    " - Working professionals have a higher conversion rates while the user count is low in comparison to unemployed\n",
    "  \n",
    " - Majority of the users have chosen better career prospects as a reason for opting for the course\n",
    " \n",
    " - The majority of the leads have been tagged under will revert after reading the email\n",
    " \n",
    " - Maximum number of leads have been from the city of mumbai followed by thane almost all categories have similar probabilities of conversion\n",
    "  \n",
    " - 90 of the leads have opted not to free copy of the interview while 31 are opted for interview copy\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Bivariate plots\n",
    "# The pair plot view shows that there is a slight correlation between total visits and page view per visit\n",
    "sns.pairplot(lead_score_df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# multivariate plots\n",
    "# The heat map shows that majority of the features have no correlation or has lesser than 0 5 correlation\n",
    "\n",
    "plt.figure(figsize = (20, 4)) # Size of the figure\n",
    "sns.heatmap(lead_score_df.select_dtypes(exclude='category').corr(), annot = True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Other Bivariate - Multivariate plots (computationally intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boxplots - numerical features against target\n",
    "\n",
    "# axs = 141\n",
    "# plt.figure(figsize=(26, 6))\n",
    "# for i in list(set(dtype_dict['float_ts']) - set(['date','converted','lead_number'])):\n",
    "#     plt.subplot(axs)\n",
    "#     sns.boxplot(y=i, x='converted', data=lead_score_df, palette='tab10')\n",
    "#     axs += 1\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Bivariate Boxplots combinations for all the categorical vs continuous columns\n",
    "\n",
    "# x_lst = list(dtype_dict['int_cat']) # x_list variable contains all the categorical Columns \n",
    "# y_lst = list(dtype_dict['float_ts'])# y_list contains all Continuous feature type columns \n",
    "# axs = 1\n",
    "# for x_col in x_lst:\n",
    "#     plt.figure(figsize=(26,72))\n",
    "#     for y_col in y_lst:\n",
    "#         plt.subplot(18,4,axs)\n",
    "#         sns.boxplot(x=x_col, y=y_col, data=lead_score_df, palette='tab10')\n",
    "#         plt.xticks(rotation=90)\n",
    "#         axs += 1\n",
    "#     plt.show();\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Bivariate Barplots combinations for all the categorical features\n",
    "\n",
    "# mplib.rcParams['ytick.labelsize'] = 8\n",
    "# mplib.rcParams['xtick.labelsize'] = 8\n",
    "\n",
    "# y_lst = x_lst = list(set(dtype_dict['int_cat']) - set(['converted','lead_number'])) # x_list variable contains all the categorical Columns \n",
    "# axs = 1\n",
    "# for x_col in x_lst:\n",
    "#     plt.figure(figsize=(25,400))\n",
    "#     for y_col in y_lst:\n",
    "        \n",
    "#         # x_f = lead_score_df[x_col]\n",
    "#         # y_f = lead_score_df[y_col]\n",
    "            \n",
    "#         # if isinstance(x_f,(object,'category')):\n",
    "#         #     x_f = x_f.astype('str',copy=True).str.slice(0,10)\n",
    "#         # if isinstance(y_f,(object,'category')):\n",
    "#         #     y_f = y_f.astype('str',copy=True).str.slice(0,10)\n",
    "        \n",
    "#         plt.subplot(60,4,axs)\n",
    "#         sns.barplot(x=x_col, y=y_col, hue ='converted' ,data=lead_score_df, palette='tab10')\n",
    "#         plt.xticks(rotation=90)\n",
    "#         # plt.ylabel(ylabel= lead_score_df[y_col].name, labelpad=-50)\n",
    "#         # plt.locator_params(tight=True)\n",
    "#         axs += 1\n",
    "#     plt.show();\n",
    "#     axs = 1\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multivariate Boxplots  - \n",
    "\n",
    "# mplib.rcParams['ytick.labelsize'] = 8\n",
    "# mplib.rcParams['xtick.labelsize'] = 8\n",
    "# # mplib.rcParams['ytick.major.size'] = 2\n",
    "# # mplib.rcParams['xtick.major.size'] = 2\n",
    "\n",
    "# z_lst = x_lst = list(dtype_dict['int_cat'])\n",
    "# y_lst = list(dtype_dict['float_ts'])\n",
    "# y_lst.remove('lead_number')\n",
    "# axs = 1\n",
    "# for x_col in x_lst:\n",
    "#     for y_col in y_lst:\n",
    "#         plt.figure(figsize=(25, 200))\n",
    "#         for z_col in z_lst:\n",
    "            \n",
    "#             # x_f = lead_score_df[x_col]\n",
    "#             # y_f = lead_score_df[y_col]\n",
    "            \n",
    "#             # if isinstance(x_f,(object,'category')):\n",
    "#             #     x_f = x_f.astype('str',copy=True).str.slice(0,10)\n",
    "#             # if isinstance(y_f,(object,'category')):\n",
    "#             #     y_f = y_f.astype('str',copy=True).str.slice(0,10)\n",
    "            \n",
    "#             plt.subplot(25, 2, axs)\n",
    "#             sns.boxplot(x=x_col, y=y_col, hue=z_col,hue_order=lead_score_df[z_col].value_counts().head(10).index ,data=lead_score_df, palette='tab10')\n",
    "#             plt.xticks(rotation=90)\n",
    "#             # plt.locator_params(tight=True)\n",
    "#             plt.legend(loc='best')\n",
    "#             axs += 1\n",
    "#         plt.show();\n",
    "#     axs = 1\n",
    "#     print(\"--------------------------------------------------------------------------\")\n",
    "#     # break\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multivariate Boxplots  - \n",
    "\n",
    "# mplib.rcParams['ytick.labelsize'] = 8\n",
    "# mplib.rcParams['xtick.labelsize'] = 8\n",
    "# # mplib.rcParams['ytick.major.size'] = 2\n",
    "# # mplib.rcParams['xtick.major.size'] = 2\n",
    "\n",
    "# x_lst = list(dtype_dict['int_cat'])\n",
    "# x_lst.remove('lead_origin')\n",
    "# x_lst.remove('converted')\n",
    "# z_lst = y_lst= x_lst\n",
    "# axs = 1\n",
    "# for x_col in x_lst:\n",
    "#     for y_col in y_lst:\n",
    "#             plt.figure(figsize=(25, 500))\n",
    "#             for z_col in z_lst:\n",
    "#                  if (x_col != y_col) & (y_col != z_col) & (x_col != z_col):\n",
    "                         \n",
    "#                 #     x_f = lead_score_df[x_col]\n",
    "#                 #     y_f = lead_score_df[y_col]\n",
    "#                 #     if isinstance(x_f,(object,'category')):\n",
    "#                 #             x_f = x_f.astype('str',copy=True).str.slice(0,10)\n",
    "#                 #     if isinstance(y_f,(object,'category')):\n",
    "#                 #             y_f = y_f.astype('str',copy=True).str.slice(0,10)\n",
    "                            \n",
    "#                     plt.subplot(100, 3, axs)\n",
    "#                     sns.barplot(x=x_col, y=y_col, hue=z_col, hue_order=lead_score_df[z_col].value_counts().head(10).index, data=lead_score_df, palette='tab10')\n",
    "#                     plt.xticks(rotation=90)\n",
    "#                 #     plt.ylabel(labelpad=3)\n",
    "#                 #     plt.locator_params.set_label_coords(-0.01, 0.5)\n",
    "#                     plt.legend(loc='best')\n",
    "#                     axs += 1\n",
    "#             plt.show()\n",
    "#             axs = 1\n",
    "#     print(\"--------------------------------------------------------------------------\")\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preprocesing - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Analysis and Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After performing eda we identified outliers in page view per visit totalvisits ttime on site columns we therefore cap all the values to the upper cut off and lower cut off of the iqr range\n",
    "\n",
    "ex_val_df = get_extremeval_threshld(df=lead_score_df.select_dtypes(exclude=['category','object']) )\n",
    "ex_val_df\n",
    "lead_score_df.describe(percentiles=[.05,.1,.2,.5,.8,.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Outliers by setting either thresh low or thresh low for both extremes \n",
    "lower_cutoff = ex_val_df.loc['pg_view_pv','thresh_low']\n",
    "lead_score_df['pg_view_pv'] = np.where((lead_score_df['pg_view_pv'] < lower_cutoff), lower_cutoff, lead_score_df['pg_view_pv'])\n",
    "upper_cutoff = ex_val_df.loc['pg_view_pv','thresh_high']\n",
    "lead_score_df['pg_view_pv'] = np.where((lead_score_df['pg_view_pv'] > upper_cutoff), upper_cutoff, lead_score_df['pg_view_pv'])\n",
    "\n",
    "# Fix Outliers by setting either thresh low or thresh low for both extremes\n",
    "lower_cutoff = ex_val_df.loc['totalvisits','thresh_low']\n",
    "lead_score_df['totalvisits'] = np.where((lead_score_df['totalvisits'] < lower_cutoff), lower_cutoff, lead_score_df['totalvisits'])\n",
    "upper_cutoff = ex_val_df.loc['totalvisits','thresh_high']\n",
    "lead_score_df['totalvisits'] = np.where((lead_score_df['totalvisits'] > upper_cutoff), upper_cutoff, lead_score_df['totalvisits'])\n",
    "\n",
    "# Fix Outliers by setting either thresh low or thresh low for both extremes\n",
    "lower_cutoff = ex_val_df.loc['ttime_on_site','thresh_low']\n",
    "lead_score_df['ttime_on_site'] = np.where((lead_score_df['ttime_on_site'] < lower_cutoff), lower_cutoff, lead_score_df['ttime_on_site'])\n",
    "upper_cutoff = ex_val_df.loc['ttime_on_site','thresh_high']\n",
    "lead_score_df['ttime_on_site'] = np.where((lead_score_df['ttime_on_site'] > upper_cutoff), upper_cutoff, lead_score_df['ttime_on_site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally there are few null values in their total visits and page view pub features therefore we replace with the mean of that particular column\n",
    "\n",
    "lead_score_df['totalvisits'] = lead_score_df['totalvisits'].replace(to_replace=np.nan, value=lead_score_df['totalvisits'].mean())\n",
    "lead_score_df['pg_view_pv'] = lead_score_df['pg_view_pv'].replace(to_replace=np.nan, value=lead_score_df['pg_view_pv'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Yes, No with 1 and 0\n",
    "lead_score_df = lead_score_df.replace(to_replace=['Yes', 'No'], value=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our null values have significantly reduced\n",
    "check_cols_null_pct(lead_score_df)\n",
    "lead_score_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.last_activity = lead_score_df.last_activity.fillna(lead_score_df.last_activity.mode()[0])\n",
    "lead_score_df.last_activity.value_counts(normalize=True) * 100\n",
    "\n",
    "lead_score_df.lead_source = lead_score_df.lead_source.fillna(lead_score_df.lead_source.mode()[0])\n",
    "lead_score_df.lead_source.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead_score_df.to_csv('spec_tag_analysis.csv')\n",
    "\n",
    "lead_score_df = lead_score_df.drop(['specialization','tags'], axis=1)\n",
    "check_cols_null_pct(lead_score_df)\n",
    "lead_score_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_score_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Imbalance & Conversion Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imbalance\n",
    "# From the target variable we have found out the imbalance ratios around 62 therefore we decide not to rebalance\n",
    "\n",
    "imbalance_ratio = sum(lead_score_df['converted'] == 1)/sum(lead_score_df['converted'] == 0) * 100\n",
    "print(f'{round(imbalance_ratio, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion Ratio \n",
    "# From the target variable the conversion ratio is around 38 it shows that there is a very high probability of failure in conversion\n",
    "\n",
    "converted = (sum(lead_score_df['converted'])/len(lead_score_df['converted'].index))*100\n",
    "print(f'{round(converted, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorline # do no remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach - 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we perform dummy encoding\n",
    "new_ls_df = pd.get_dummies(lead_score_df, columns=lead_score_df.select_dtypes('category').columns, drop_first=True, dtype=float)\n",
    "new_ls_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_ls_df.drop(['converted'], axis=1)\n",
    "y = new_ls_df['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the dataset into train and test set\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ls_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post split we perform standard scaling they fit and transform the train data set\n",
    "\n",
    "# to_scale = ['lead_number', 'totalvisits', 'ttime_on_site', 'pg_view_pv']\n",
    "to_scale = list(X.columns)\n",
    "to_scale\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[to_scale] = scaler.fit_transform(X_train[to_scale], y_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom functions for model veiling since iteration we reuse certain functions again and again\n",
    "# Train and predict function trains the model and predicts on the same data and returns the model its probability and predicted values based on cutoff\n",
    "# The matrix function returns confusion matrix and accuracy score\n",
    "# The vif function returns the vif score for the features\n",
    "\n",
    "def logreg_train_pred_fn(fX_train, fy_train, fcol, fcutoff):\n",
    "    fX_train_sm = sm.add_constant(fX_train[fcol])\n",
    "    flogm = sm.GLM(fy_train, fX_train_sm, family = sm.families.Binomial())\n",
    "    fres = flogm.fit()\n",
    "    fy_train_pred = fres.predict(fX_train_sm)\n",
    "    fy_train_pred = fy_train_pred.values.reshape(-1)\n",
    "    fy_train_pred_final = pd.DataFrame({'Converted':fy_train.values, 'Conv_Prob':fy_train_pred})\n",
    "    fy_train_pred_final['ID'] = fy_train.index\n",
    "    fy_train_pred_final['predicted'] = fy_train_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_train_pred,fy_train_pred_final\n",
    "\n",
    "def logreg_metrics_fn(fy_train_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_train_pred_final.Converted, fy_train_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_train_pred_final.Converted, fy_train_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_VIF_score_fn(fX_train, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_train[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_train[fcol].values, i) for i in range(fX_train[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE - Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set has a lot of features we perform rfe to eliminate insignificant features\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we perform model iteration as many times as possible till we get an optimum result\n",
    "\n",
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('curr_occupation_Housewife', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.35 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve and Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = roc_curve( actual, probs, drop_intermediate = False )\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, drop_intermediate = False )\n",
    "draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\n",
    "recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)\n",
    "plt.plot(thresholds, p[:-1], \"b\")\n",
    "plt.plot(thresholds, r[:-1], \"r\")\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_test_pred_fn(fX_test, fy_test, fcol, fcutoff, fres):\n",
    "    fX_test_sm = sm.add_constant(fX_test[fcol])\n",
    "    fy_test_pred = fres.predict(fX_test_sm)\n",
    "    fy_test_pred = fy_test_pred.values.reshape(-1)\n",
    "    fy_test_pred_final = pd.DataFrame({'Converted':fy_test.values, 'Conv_Prob':fy_test_pred})\n",
    "    fy_test_pred_final['ID'] = fy_test.index\n",
    "    fy_test_pred_final['predicted'] = fy_test_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_test_pred,fy_test_pred_final\n",
    "\n",
    "def logreg_test_metrics_fn(fy_test_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_test_pred_final.Converted, fy_test_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_test_pred_final.Converted, fy_test_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_test_VIF_score_fn(fX_test, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_test[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_test[fcol].values, i) for i in range(fX_test[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "X_test[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.3\n",
    "res, y_test_pred, y_test_pred_final = logreg_test_pred_fn(X_test, y_test, col, cutoff, res)\n",
    "confusion, accuracy = logreg_test_metrics_fn(y_test_pred_final)\n",
    "vif = logreg_test_VIF_score_fn(X_test, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_test_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_test_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorline # do no remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach - 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_encoding:\n",
    "# Since there are many categorical features with nominal values 5 we decide to use label encoding for those features\n",
    "\n",
    "cols_to_le = [i for i in lead_score_df.columns if ((lead_score_df[i].dtype == 'category') and len(lead_score_df[i].unique()) > 5)]\n",
    "cols_to_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_le = lead_score_df[cols_to_le].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_le.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then merged the encoder features with the non encoded features and name it as new lsdf\n",
    "\n",
    "new_ls_df = lead_score_df[lead_score_df.columns.difference(cols_to_le)]\n",
    "new_ls_df = new_ls_df.merge(right=df_le, right_index=True, left_index=True)\n",
    "new_ls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our null values have significantly reduced\n",
    "check_cols_null_pct(new_ls_df)\n",
    "new_ls_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ls_df = new_ls_df[(new_ls_df.notna()).all(axis=1)]\n",
    "# check_cols_null_pct(new_ls_df)\n",
    "# sorted([f'{i} - {new_ls_df[i].unique()}' for i in new_ls_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label_encoding:\n",
    "# For those columns that are lesser than 5 labels we perform dummy ecoding\n",
    "\n",
    "cols_to_de = [i for i in new_ls_df.columns if ((new_ls_df[i].dtype == 'category') and len(new_ls_df[i].unique()) <= 5)]\n",
    "cols_to_de\n",
    "new_ls_df = pd.get_dummies(new_ls_df, columns=cols_to_de, drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ls_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_ls_df.drop(['converted'], axis=1)\n",
    "y = new_ls_df['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the dataset into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post split we perform standard scaling they fit and transform the train data set\n",
    "\n",
    "# to_scale = ['lead_number', 'totalvisits', 'ttime_on_site', 'pg_view_pv']\n",
    "to_scale = ['totalvisits', 'ttime_on_site', 'pg_view_pv', 'lead_source', 'last_activity', 'country', 'curr_occupation']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[to_scale] = scaler.fit_transform(X_train[to_scale],y_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom functions for model veiling since iteration we reuse certain functions again and again\n",
    "# Train and predict function trains the model and predicts on the same data and returns the model its probability and predicted values based on cutoff\n",
    "# The matrix function returns confusion matrix and accuracy score\n",
    "# The vif function returns the vif score for the features\n",
    "\n",
    "def logreg_train_pred_fn(fX_train, fy_train, fcol, fcutoff):\n",
    "    fX_train_sm = sm.add_constant(fX_train[fcol])\n",
    "    flogm = sm.GLM(fy_train, fX_train_sm, family = sm.families.Binomial())\n",
    "    fres = flogm.fit()\n",
    "    fy_train_pred = fres.predict(fX_train_sm)\n",
    "    fy_train_pred = fy_train_pred.values.reshape(-1)\n",
    "    fy_train_pred_final = pd.DataFrame({'Converted':fy_train.values, 'Conv_Prob':fy_train_pred})\n",
    "    fy_train_pred_final['ID'] = fy_train.index\n",
    "    fy_train_pred_final['predicted'] = fy_train_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_train_pred,fy_train_pred_final\n",
    "\n",
    "def logreg_metrics_fn(fy_train_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_train_pred_final.Converted, fy_train_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_train_pred_final.Converted, fy_train_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_VIF_score_fn(fX_train, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_train[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_train[fcol].values, i) for i in range(fX_train[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE - Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set has a lot of features we perform rfe to eliminate insignificant features\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we perform model iteration as many times as possible till we get an optimum result\n",
    "\n",
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('lead_origin_Quick Add Form', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('reason_behind_course_Other')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('avail_free_copy_1')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('lead_origin_Lead Import')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('reason_behind_course_Flexibility & Convenience')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.3 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve and Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = roc_curve( actual, probs, drop_intermediate = False )\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, drop_intermediate = False )\n",
    "draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\n",
    "recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)\n",
    "plt.plot(thresholds, p[:-1], \"b\")\n",
    "plt.plot(thresholds, r[:-1], \"r\")\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_test_pred_fn(fX_test, fy_test, fcol, fcutoff, fres):\n",
    "    fX_test_sm = sm.add_constant(fX_test[fcol])\n",
    "    fy_test_pred = fres.predict(fX_test_sm)\n",
    "    fy_test_pred = fy_test_pred.values.reshape(-1)\n",
    "    fy_test_pred_final = pd.DataFrame({'Converted':fy_test.values, 'Conv_Prob':fy_test_pred})\n",
    "    fy_test_pred_final['ID'] = fy_test.index\n",
    "    fy_test_pred_final['predicted'] = fy_test_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_test_pred,fy_test_pred_final\n",
    "\n",
    "def logreg_test_metrics_fn(fy_test_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_test_pred_final.Converted, fy_test_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_test_pred_final.Converted, fy_test_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_test_VIF_score_fn(fX_test, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_test[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_test[fcol].values, i) for i in range(fX_test[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "X_test[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.3\n",
    "res, y_test_pred, y_test_pred_final = logreg_test_pred_fn(X_test, y_test, col, cutoff, res)\n",
    "confusion, accuracy = logreg_test_metrics_fn(y_test_pred_final)\n",
    "vif = logreg_test_VIF_score_fn(X_test, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_test_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_test_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorline # do no remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach - 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we perform dummy encoding\n",
    "\n",
    "new_ls_df = pd.get_dummies(lead_score_df, columns=lead_score_df.select_dtypes('category').columns, drop_first=True, dtype=float)\n",
    "new_ls_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_ls_df.drop(['converted'], axis=1)\n",
    "y = new_ls_df['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the dataset into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ls_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post split we perform standard scaling they fit and transform the train data set\n",
    "\n",
    "# to_scale = ['lead_number', 'totalvisits', 'ttime_on_site', 'pg_view_pv']\n",
    "to_scale = list(set(list(X.columns)) - set(['totalvisits','ttime_on_site','pg_view_pv']))\n",
    "to_scale\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[to_scale] = scaler.fit_transform(X_train[to_scale], y_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create custom functions for model veiling since iteration we reuse certain functions again and again\n",
    "# Train and predict function trains the model and predicts on the same data and returns the model its probability and predicted values based on cutoff\n",
    "# The matrix function returns confusion matrix and accuracy score\n",
    "# The vif function returns the vif score for the features\n",
    "\n",
    "def logreg_train_pred_fn(fX_train, fy_train, fcol, fcutoff):\n",
    "    fX_train_sm = sm.add_constant(fX_train[fcol])\n",
    "    flogm = sm.GLM(fy_train, fX_train_sm, family = sm.families.Binomial())\n",
    "    fres = flogm.fit()\n",
    "    fy_train_pred = fres.predict(fX_train_sm)\n",
    "    fy_train_pred = fy_train_pred.values.reshape(-1)\n",
    "    fy_train_pred_final = pd.DataFrame({'Converted':fy_train.values, 'Conv_Prob':fy_train_pred})\n",
    "    fy_train_pred_final['ID'] = fy_train.index\n",
    "    fy_train_pred_final['predicted'] = fy_train_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_train_pred,fy_train_pred_final\n",
    "\n",
    "def logreg_metrics_fn(fy_train_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_train_pred_final.Converted, fy_train_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_train_pred_final.Converted, fy_train_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_VIF_score_fn(fX_train, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_train[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_train[fcol].values, i) for i in range(fX_train[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "res = logm1.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE - Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data set has a lot of features we perform rfe to eliminate insignificant features\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(estimator=logreg, n_features_to_select=15)             # running RFE with 15 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we perform model iteration as many times as possible till we get an optimum result\n",
    "\n",
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')       # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('curr_occupation_Housewife', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('country_Canada', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('country_Qatar', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = col.drop('last_activity_Email Marked Spam', 1)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.5\n",
    "res, y_train_pred,y_train_pred_final = logreg_train_pred_fn(X_train, y_train, col, cutoff)\n",
    "confusion, accuracy = logreg_metrics_fn(y_train_pred_final)\n",
    "vif = logreg_VIF_score_fn(X_train, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_train_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_train_pred_final\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')   # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.35 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve and Precision - Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = roc_curve( actual, probs, drop_intermediate = False )\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conv_Prob, drop_intermediate = False )\n",
    "draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\n",
    "recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conv_Prob)\n",
    "plt.plot(thresholds, p[:-1], \"b\")\n",
    "plt.plot(thresholds, r[:-1], \"r\")\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_test_pred_fn(fX_test, fy_test, fcol, fcutoff, fres):\n",
    "    fX_test_sm = sm.add_constant(fX_test[fcol])\n",
    "    fy_test_pred = fres.predict(fX_test_sm)\n",
    "    fy_test_pred = fy_test_pred.values.reshape(-1)\n",
    "    fy_test_pred_final = pd.DataFrame({'Converted':fy_test.values, 'Conv_Prob':fy_test_pred})\n",
    "    fy_test_pred_final['ID'] = fy_test.index\n",
    "    fy_test_pred_final['predicted'] = fy_test_pred_final.Conv_Prob.map(lambda x: 1 if x > fcutoff else 0)\n",
    "    return fres, fy_test_pred,fy_test_pred_final\n",
    "\n",
    "def logreg_test_metrics_fn(fy_test_pred_final):\n",
    "    fconfusion = confusion_matrix(fy_test_pred_final.Converted, fy_test_pred_final.predicted )\n",
    "    faccuracy = accuracy_score(fy_test_pred_final.Converted, fy_test_pred_final.predicted)\n",
    "    return fconfusion, faccuracy\n",
    "    \n",
    "def logreg_test_VIF_score_fn(fX_test, fcol):\n",
    "    fvif = pd.DataFrame()\n",
    "    fvif['Features'] = fX_test[fcol].columns\n",
    "    fvif['VIF'] = [variance_inflation_factor(fX_test[fcol].values, i) for i in \n",
    "                   range(fX_test[fcol].shape[1])]\n",
    "    fvif['VIF'] = round(fvif['VIF'], 2)\n",
    "    fvif = fvif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return fvif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "X_test[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.3\n",
    "res, y_test_pred, y_test_pred_final = logreg_test_pred_fn(X_test, y_test, col, cutoff, res)\n",
    "confusion, accuracy = logreg_test_metrics_fn(y_test_pred_final)\n",
    "vif = logreg_test_VIF_score_fn(X_test, col)\n",
    "\n",
    "print('Model Summary:')          # Model Summary:\n",
    "res.summary()\n",
    "print('\\nVIF Score:')            # VIF Score:\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nY_Predicted Values:')   # Y_Predicted Values:\n",
    "# y_test_pred\n",
    "# print('\\nY_Predicted Cutoff:')   # Y_Predicted Cutoff:\n",
    "# y_test_pred_final\n",
    "\n",
    "print('\\nConfusion Matrix:')     # Confusion Matrix: \n",
    "confusion\n",
    "print(f'\\nAccuracy Score: {accuracy}\\n')  # Accuracy Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfull",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
